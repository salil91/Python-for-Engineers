{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c992e8-ce79-4544-bf59-cea68420dc32",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 6 - Machine Learning\n",
    "**Due: Nov 28** \n",
    "\n",
    "***Total Points: 100***\n",
    "\n",
    "For full points, your code\n",
    "- must run without errors\n",
    "- must by *pythonic*\n",
    "- must be easily understandable, and well documented (either through inline comments or markdown).\n",
    "- All plots must have clearly and meaningfully labeled axes, unless you are simply plotting arbitrary numbers. Add legends where needed.\n",
    "- **Use separate markdown cells for any text answers.**\n",
    "- **Show your work, i.e, print all relevant output.** Remember that having a variable in the last line of a jupyter cell block automatically prints it's value. You can use multiple code blocks for a single question.\n",
    "- ***Please see the solutions (and related notes) for the previous HWs, as well as the comments on your submissions, before submitting this assignment. We will be less lenient for repeated mistakes.***\n",
    "\n",
    "Remember to export your Jupyter notebook as a PDF file and upload both to Canvas.\n",
    "```\n",
    "File > Save and Export Notebook As... > PDF\n",
    "```\n",
    "\n",
    "Run every code block (and make sure the answer if fully visible) before submitting your notebook/PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e72583-fbcf-44a1-942d-dd95e5d9b9da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 0\n",
    "\n",
    "Part of this HW covers the missed lecture. The examples are derived from the [SciPy lecture Notes](https://scipy-lectures.org/packages/scikit-learn/index.html#parameter-selection-validation-and-testing), which are, in turn, adapted from [Andrew NG's Supervised Machine Learning course on Coursera](https://www.coursera.org/learn/machine-learning).\n",
    "\n",
    "Selecting the optimal model for your data is vital, and is a piece of the problem that is not often appreciated by machine learning practitioners.\n",
    "\n",
    "The central question is: If our estimator is underperforming, how should we move forward?\n",
    "- Use simpler or more complicated model?\n",
    "- Add more features to each observed data point?\n",
    "- Add more training samples?\n",
    "\n",
    "The answer is often counter-intuitive. In particular, Sometimes using a more complicated model will give worse results. Also, sometimes adding training data will not improve your results. The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.\n",
    "\n",
    "We will explore these questions below :)\n",
    "\n",
    "**Your task:**\n",
    "Import `numpy` and your plotting package(s) of choice here. You can import the required `sklearn` modules when you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9655bf-603a-4a7d-a6bc-97889f9f7eb0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7500a64a-cdbd-40bd-a6e3-efc1e3aebb72",
   "metadata": {},
   "source": [
    "## Question 1: Polynomial regression with `sklearn`\n",
    "*15 points*\n",
    "\n",
    "We are going to generate some data and fit a polynomial to it by creating a pipeline between `PolynomialFeatures` and `LinearRegression`.\n",
    "- `PolynomialFeatures` generates a polynomial and its interaction features.\n",
    "- `make_pipeline` feeds the output of the first step (argument) as the input to the second step (argument).\n",
    "\n",
    "**Your tasks:**\n",
    "- Plot the function below, for the given `x_train` values\n",
    "- Perform a 3rd order polynomial regression using `x_train` values and the 3 functions/classes listed above.\n",
    "- On the same figure, plot the fit using `x_test` values, and make sure you include the labels and legend. Set the `ylim` for the plot to be `(0, 12)`.\n",
    "\n",
    "The documentation is your friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f028642-abc7-4289-80df-8f86579d8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code block without changes\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def generate_data(x, err=0.5):\n",
    "    return np.random.normal(10 - 1. / (x + 0.1), err)\n",
    "\n",
    "x_train = 10 ** np.linspace(-2, 0, 8)\n",
    "y_train = generate_data(x_train)\n",
    "x_test = np.linspace(-0.2, 1.2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073182f0-f60f-46a9-92c0-1bafa69a375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2736608c-548a-42b2-8abb-6bbe6b8d59dd",
   "metadata": {},
   "source": [
    "## Question 2: Hyperparameter selection, over-fitting, and under-fitting\n",
    "*15 points*\n",
    "\n",
    "Typically, when selecting a model, there is a trade-off between bias and variance. Both are prediction errors that we want to minimize.\n",
    "- **Bias** is the error between the average predicted value and the expected (true) value. A model with high bias does not pay enough attention to the training data. It oversimplifies the model, and **underfits** the data, leading to a high error on training and test data. In engineering terms, it has poor *accuracy*.\n",
    "- **Variance** is the variability of in the predicted values for a given data point, i.e., the spread of our data. A  model with high variance pays too much attention to training data and **overfits** it. Hence, is not able to perform well on data it hasnâ€™t seen before. As a result, it performs very well on training data but has high error rates on test data. In engineering terms, it has poor *precision*.\n",
    "\n",
    "![precision and bias](https://miro.medium.com/max/640/1*xwtSpR_zg7j7zusa4IDHNQ.png)\n",
    "\n",
    "**Choosing the right tradeoff between bias and variance leads to the best prediction performance.**\n",
    "\n",
    "Model choice affects this tradeoff. For example, non-regularized estimators (linear regression) are not biased, but they can display a lot of variance. Highly-regularized models (ridge regression) have little variance, but high bias.\n",
    "\n",
    "Choosing the right hyperparameter (e.g., `alpha`, which sets the strength of the regularization) also affects the tradeoff. For a specific dataset there is a sweet spot corresponding to the highest complexity that the data can support, depending on the amount of noise and of observations available.\n",
    "\n",
    "For polynomial regression, the hyperparameter is the `degree` of the polynomial.\n",
    "\n",
    "**Your tasks:**\n",
    "- Repeat the problem from Question 1, but use a 1st, 2nd, and 6th order polynomial regression.\n",
    "- Produce all plots on a single `Figure` with 3 subplots in a single row. Set the `ylim` for each to be `(0, 12)`. Use a single x and y axis label.\n",
    "- Comment on the results. Which model has a high bias or variance? Which one is an underfit or an overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dd498-ecc3-4511-8d85-b45ef3d7b755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39f5a7c-d513-44d8-be51-468b83669764",
   "metadata": {},
   "source": [
    "## Question 3: Validation curves\n",
    "*15 points*\n",
    "\n",
    "By plotting your fits above, you should have been able to *qualitatively* determine the bias and variance for each model. Let's now look at *quantifying* these using a validation curve.\n",
    "\n",
    "**Validation curves shows the model score (e.g., $R^2$) as a function of a *model hyperparameter***. The hyperparameter that controls the model complexity (e.g., `polynomialfeatures__degree`) is varied and the mean model score for the training data and on the test data (cross-validation) is plotted.\n",
    "\n",
    "Remember:\n",
    "$$R^2 = \\frac{SSR}{SST} = \\frac{\\text{Explained variability}}{\\text{Total variability}}$$\n",
    "\n",
    "![Regression errors](https://365datascience.com/resources/blog/2018-11-image8-5-1024x495.jpg)\n",
    "\n",
    "Source: https://365datascience.com/tutorials/statistics-tutorials/sum-squares/\n",
    "\n",
    "A low model score (i.e., a low explained variance) for both the training set and the cross-validation set indicates that the model under-fits the data. It is too simple. A large difference between the training score and the cross-validation score indicates that the model over-fits the data. It is good at predicting on data it has seen, but bad at predicting on new data.\n",
    "\n",
    "**Your tasks:**\n",
    "- Create a `train_test_split` on the `x` and `y` data below, using a `test_size=0.4`. Plot the train and test data, using different colors for each.\n",
    "- Use `sklearn.model_selection.validation_curve()` to compute the train and test variance for models with polynomial degrees ranging from 1 to 15. Plot the validation curve using the mean variance for each hyperparameter value (i.e., as a function of polynomial degree).\n",
    "- Comment on the results. Which region of the curve shows underfitting, and which shows over-fitting? What is the best value for the degree hyperparameter?\n",
    "\n",
    "Hint: Since we are using a pipeline, you can set the `param_name` argument for `validation_curve()` using the double underscore notation: `param_name=\"polynomialfeatures__degree\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adedcb42-4d2e-44bd-8582-e02b7b96fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code block without changes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "x = np.random.random(size=200)\n",
    "y = generate_data(x, err=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824d51d-a70a-453d-9a8c-491a27fd4cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce2facf-b151-4639-9890-b9a9b373b216",
   "metadata": {},
   "source": [
    "## Question 4: Learning curves\n",
    "*15 points*\n",
    "\n",
    "In general, model complexity (degree of polynomial) can be increased with the number of training points used. Learning curves give a quantitative view into how model performance improves when adding training samples to the data.\n",
    "\n",
    "**Learning curves show the training and validation score as a function of the *number of training points***. When training on a subset of the training data, the training score is computed using this subset, not the full training set.\n",
    "\n",
    "The validation score generally increases with a growing training set, while the training score generally decreases with a growing training set. Hence, as the training size increases, they will converge to a single value.\n",
    "- When the learning curves converge to a low score, we have a *high bias* model. Since this model *under-fits* the data, it makes sense that adding more training data will not help. \n",
    "- When the training score is much higher than the validation score, we have a *high-variance* model. Since this model *over-fits* the data, adding more samples to the training set can help improve the model. The training score will continue to decrease, while the cross-validation error will continue to increase, until they meet in the middle.\n",
    "\n",
    "**Your tasks:**\n",
    "- Create polynomial models using a 1st and 15th order polynomial, as well as the \"best value\" for the degree that you determined above.\n",
    "- Use `sklearn.model_selection.learning_curve()` to generate the learning curves for each model and plot them as subplots using 3 columns. Set the `ylim` for each to be `(0, 1)`. Use a single x and y axis label, and a single legend.\n",
    "- Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882545a-d0f3-4dca-bcfb-988dd1049912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code block without changes\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Use the following for the train_sizes argument in the learning_curve() function\n",
    "train_sizes=np.logspace(-1, 0, 20)\n",
    "\n",
    "x = np.random.random(size=200)\n",
    "y = generate_data(x, err=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ee346-1064-4dd9-b752-dfd88b52a3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461c02f6-f71e-4d52-aa0a-3ac2c65ef5cc",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "**A high-bias model can be improved by:**\n",
    "- Using a more sophisticated model (i.e. for a polynomial, increase the degree).\n",
    "- Decrease regularization in a regularized model.\n",
    "- Gather more features for each sample.\n",
    "\n",
    "*Increasing the number of samples, however, does not improve a high-bias model.* In fact, reducing the number of samples will likely have have the same error, but at a lower computational cost.\n",
    "\n",
    "**A low-bias model can be improved by:**\n",
    "- Using a less-sophisticated model (i.e. for a polynomial, decrease the degree).\n",
    "- Increasing regularization in a regularized model.\n",
    "- Increase the number training samples (by gathering more, not by reducing the test size).\n",
    "\n",
    "*Gathering more features, however, does not improve a high-variance model.* In fact, using fewer features reduce the effect of over-fitting.\n",
    "\n",
    "## A final note\n",
    "\n",
    "Determining hyper-parameters using these validation schemes means that we are fitting the hyper-parameters to the particular validation set. In the same way that parameters can be over-fit to the training set, hyperparameters can be over-fit to the validation set. Because of this, the validation error tends to under-predict the error for new data. To gauge the error of a model on unknown data, using an independent test set is vital.\n",
    "\n",
    "Hence, it is recommended to split the data into three sets:\n",
    "- The test set, used to evaluate the expected error of the validated model (usually ~20% of the data, `test_size=0.2`)\n",
    "- The validation set, used to validate the model (usually ~20% of the data, `cv=4` or `n_splits=4`)\n",
    "- The training set, used to train the model (usually ~60% of the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaec24-c693-40c3-8088-4a41bc09c5ba",
   "metadata": {},
   "source": [
    "## Question 5: Regression with Cross-validation\n",
    "*40 points*\n",
    "\n",
    "Using everything we have learned in class and the previous questions, let's do some Machine Learning on some material data given in the `elastic_modulus.csv` file. The target variable is the `elastic_modulus`.\n",
    "\n",
    "**Your tasks:**\n",
    "- Load the dataset, separate out the target from the features, and visualize the target as a function of each feature.\n",
    "- Standardize the data, and visualize it again. *HINT: there is a `StandardScaler` function in scikit-learn*\n",
    "- Reserve a test set using 20% of the original data to check results after optimal regressor is found. *HINT: See \"A final note\" above.*\n",
    "- Using the `RidgeCV` and `LassoCV` models, find the optimal hyperparameter value for each model. *HINT: try values of alpha ranging from $10^{-4}$ to $10^2$*\n",
    "- Using nested cross-validation, try and find the model which provides the best fit. What other metric could you use to select the \"best\" model?\n",
    "- With the better of the two models, find the RMS error on the reserved test set. You should not have touched the test set up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d18b7f-d3fb-4070-907e-94695fe54f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9fde0f4dbc602a88a135b77e70b819c5fd445d8608dce2e5fdc6c2a0529b12b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
